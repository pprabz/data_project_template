ğŸ“˜ Data Project Template

A reusable, professional template for data analysis, EDA, modeling, and visualization workflows.

This repository provides a standard folder structure, starter notebooks, and requirements for any data project.
It ensures consistency, clean organization, and reproducibility â€” aligned with how companies structure data science projects.

ğŸš€ Features of This Template
âœ” Predefined folder structure
project/
â”œâ”€â”€ data_raw/          â†’ Original untouched datasets  
â”œâ”€â”€ data_processed/    â†’ Cleaned / transformed datasets  
â”œâ”€â”€ notebooks/         â†’ Ready-to-use Jupyter templates  
â”œâ”€â”€ scripts/           â†’ Reusable Python utilities & SQL cleaning script  
â”œâ”€â”€ outputs/           â†’ Charts, reports, model files  
â””â”€â”€ config/            â†’ requirements.txt and environment-related files

âœ” Ready-to-run notebooks

01_eda_template.ipynb â€“ Exploratory Data Analysis

02_modeling_template.ipynb â€“ ML starter pipeline

03_visualization_template.ipynb â€“ Plotting & dashboards

âœ” Optimized requirements.txt

Includes: pandas, numpy, sklearn, matplotlib, seaborn, duckdb, pyarrow, profiling tools, etc.

âœ” Git-friendly

.gitignore file included

Raw data folders excluded from version control

ğŸ—ï¸ How This Template Was Created

Creating template was built to provide a clean, repeatable structure for data projects.
Steps used to create it:
1ï¸âƒ£ Create a reusable template folder
In Termial

```bash
mkdir -p ~/templates/data_project_template
cd ~/templates/data_project_template

mkdir -p data_raw data_processed notebooks scripts outputs config

```

Created folder structure:
```text
data_raw/
data_processed/
notebooks/
scripts/
outputs/
config/
```
** This folder will NEVER be used directly for work â€” only copied.

2ï¸âƒ£ Add a reusable requirements.txt
```bash
nano config/requirements.txt
```
```text
pandas
numpy
matplotlib
seaborn
scikit-learn
pyarrow
openpyxl
jupyterlab
```
Save (Ctrl+O, Enter) and exit (Ctrl+X).

3ï¸âƒ£ Add your EDA notebook template

Activate your env & open Jupyter in the template folder:
```bash
conda activate aihc_env
cd ~/templates/data_project_template
jupyter lab
```


In JupyterLab:

Go to notebooks/

Create a new notebook

create and Save it as:
01_eda_template.ipynb

Thatâ€™s now your master EDA template.

Added:

Templates for EDA, modeling, visualization

requirements.txt

utils.py + SQL cleaning script

README.md

.gitignore to avoid committing datasets

Uploaded the folder to GitHub as a Template Repository
(GitHub â†’ Settings â†’ General â†’ Enable â€œTemplate repositoryâ€)

This allows anyone to click â€œUse this templateâ€ to start a new project instantly.

ğŸ“¦ How to Use This Template (Step by Step)
1ï¸âƒ£ Create a new project from the template

Go to this repository on GitHub â†’ Click:

Use this template â†’ Create a new repository


Example new repository name:

customer-analysis-2025


This creates a clean copy of the template as a new project.

2ï¸âƒ£ Clone your new project to your computer
cd ~/projects
git clone https://github.com/YOUR_USERNAME/customer-analysis-2025.git
cd customer-analysis-2025

3ï¸âƒ£ Activate your Conda environment
`conda activate aihc_env

4ï¸âƒ£ Install all required libraries
`pip install -r config/requirements.txt


This installs:

pandas, numpy, matplotlib

seaborn, sklearn

duckdb, SQLAlchemy

ydata-profiling, missingno

jupyterlab
â€¦and more.

5ï¸âƒ£ Add Data

Place your raw dataset files in:

data_raw/


Example:

data_raw/customers.csv

6ï¸âƒ£ Start working using Jupyter
jupyter lab


Start with:

notebooks/01_eda_template.ipynb

Update DATA_PATH inside the notebook

Run the full EDA pipeline

Then continue with:

02_modeling_template.ipynb

03_visualization_template.ipynb

ğŸ”„ Sync Changes Back to GitHub (Version Control Workflow)

Inside your project folder:

1. Check what changed
git status

2. Add changes
git add .

3. Commit those changes
git commit -m "Describe what you changed: e.g., added EDA and cleaned data"

4. Push changes to GitHub
git push


Now GitHub gets updated with:

your notebooks

your scripts

your visualizations

your progress

(Datasets will not be uploaded because they live in data_raw/ and are ignored â€” which is good.)

ğŸ§­ Typical Company-Style Workflow

Create new project from template

Clone locally

Create a clean branch (optional but recommended)

Activate environment

Install dependencies

Add data to data_raw/

Perform EDA

Clean & transform data â†’ save in data_processed/

Feature engineering

Use modeling or visualization template

Commit & push progress often

Share results or merge branches

Repeat process for next project

ğŸ§± Best Practices
âœ” Never edit the original template; clone from GitHub each time
âœ” Never commit datasets (template already ignores them)
âœ” Always write meaningful commit messages
âœ” Keep notebooks clean and organized
âœ” Keep reusable functions in scripts/utils.py
âœ” Export cleaned CSVs to data_processed/
âœ” Use one Conda environment (aihc_env) for all projects
ğŸ¤ Contributing / Improving the Template

If you improve:

the EDA process

modeling code

visualization workflow

utils scripts

requirements.txt

â€¦then update this template repo, so all future projects benefit.

You can either:

Push changes directly (if you own the repo)

Or fork â†’ improve â†’ open a pull request

ğŸ‰ Summary

This template helps you:

Start every project quickly

Maintain consistent structure

Avoid messy folders

Reuse a tested workflow

Stay aligned with data engineering best practices

Easily collaborate on GitHub

Keep raw and processed data separate

Make your work reproducible